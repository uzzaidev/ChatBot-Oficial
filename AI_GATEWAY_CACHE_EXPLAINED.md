# Vercel AI Gateway - Cache Explicado

> **Status:** ‚úÖ Implementado e funcionando
> **√öltima atualiza√ß√£o:** 16 de Dezembro de 2024

---

## üìö √çndice

1. [O que √© o Vercel AI Gateway](#o-que-√©-o-vercel-ai-gateway)
2. [Tipos de Cache](#tipos-de-cache)
3. [Como Funciona o Prompt Caching](#como-funciona-o-prompt-caching)
4. [Configura√ß√£o Atual](#configura√ß√£o-atual)
5. [Benef√≠cios](#benef√≠cios)
6. [Monitoramento](#monitoramento)
7. [Exemplos Pr√°ticos](#exemplos-pr√°ticos)
8. [FAQ](#faq)

---

## O que √© o Vercel AI Gateway

O **Vercel AI Gateway** √© uma camada intermedi√°ria entre sua aplica√ß√£o e os provedores de IA (OpenAI, Anthropic, Groq, Google). Ele oferece:

- ‚úÖ **Dashboard de monitoramento** (requests, custos, lat√™ncia)
- ‚úÖ **Prompt Caching** (economiza tokens/custos)
- ‚úÖ **Fallback autom√°tico** (se um provider falhar)
- ‚úÖ **Telemetria** (rastreamento detalhado)
- ‚úÖ **Centraliza√ß√£o** (um √∫nico endpoint)

**Arquitetura:**

```
WhatsApp ‚Üí Webhook ‚Üí chatbotFlow ‚Üí AI Gateway ‚Üí Provider (OpenAI/Groq/etc)
                                        ‚Üì
                                  Vercel Dashboard
                                  (monitoramento)
```

---

## Tipos de Cache

### ‚ùå Response Caching (O que N√ÉO temos)

**O que √©:**
- Cacheia a **resposta completa** da IA
- Se a mesma pergunta for feita, retorna a resposta armazenada
- N√£o chama a LLM novamente

**Exemplo:**
```javascript
Cliente 1: "Qual o hor√°rio?"
Bot: [Chama LLM] ‚Üí "Atendemos das 9h √†s 18h"
üíæ Salva: pergunta ‚Üí resposta

Cliente 2: "Qual o hor√°rio?" (MESMA pergunta)
Bot: [N√£o chama LLM] ‚Üí "Atendemos das 9h √†s 18h" (cache!)
‚ö° Instant√¢neo (0ms) + Gr√°tis
```

**Benef√≠cios:**
- ‚ö° Velocidade: resposta instant√¢nea
- üí∞ Custo: zero (n√£o chama LLM)

**Limita√ß√µes:**
- S√≥ funciona para perguntas EXATAMENTE iguais
- Respostas podem ficar desatualizadas
- Precisa implementar manualmente (Redis, etc)

---

### ‚úÖ Prompt Caching (O que a Vercel oferece)

**O que √©:**
- Cacheia **parte do prompt** (system message, RAG context)
- N√£o paga novamente por tokens repetidos
- **MAS ainda chama a LLM** para gerar resposta

**Exemplo:**
```javascript
Cliente 1: "Qual o hor√°rio?"
LLM recebe:
‚îú‚îÄ System: "Voc√™ √© assistente..." (500 tokens) ‚îÄ‚îÄ‚îê
‚îú‚îÄ RAG: "Hor√°rio 9-18h, tel 123..." (1000 tokens)‚îÇ Processados
‚îú‚îÄ Hist√≥rico: "..." (200 tokens)                  ‚îÇ e pagos
‚îî‚îÄ User: "Qual o hor√°rio?" (5 tokens)            ‚îÇ
üí∞ Total: 1705 tokens pagos ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Cliente 1 (pr√≥xima mensagem): "E o telefone?"
LLM recebe:
‚îú‚îÄ System: "Voc√™ √© assistente..." (500 tokens) ‚îÄ‚îÄ‚îê üíæ CACHED!
‚îú‚îÄ RAG: "Hor√°rio 9-18h, tel 123..." (1000 tokens)‚îÇ N√£o paga
‚îú‚îÄ Hist√≥rico: "..." (200 tokens)                  ‚îÇ de novo
‚îî‚îÄ User: "E o telefone?" (5 tokens)              ‚îÇ
üí∞ Total: 5 tokens pagos ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
üìâ Economia: 99% nos tokens de input!
```

**Benef√≠cios:**
- üí∞ Custo: 60-70% de economia em tokens
- üìä Dashboard: visibilidade completa
- üîÑ Autom√°tico: sem c√≥digo adicional

**Limita√ß√µes:**
- ‚è±Ô∏è Tempo: mesma lat√™ncia (ainda gera resposta)
- üîó Prefixo: s√≥ funciona se in√≠cio do prompt for igual

---

## Como Funciona o Prompt Caching

### Regras do Cache

O cache **S√ì funciona** se:

1. ‚úÖ **M√≠nimo de 1024 tokens** - OpenAI requer pelo menos 1024 tokens no prompt! ([Ref](https://platform.openai.com/docs/guides/prompt-caching))
2. ‚úÖ **Prefixo id√™ntico** - In√≠cio do array de mensagens deve ser IGUAL
3. ‚úÖ **Mesma ordem** - N√£o pode reordenar mensagens
4. ‚úÖ **Mesmo conte√∫do** - Nem um caractere diferente
5. ‚úÖ **Par√¢metros iguais** - temperature, model, etc

**‚ö†Ô∏è CR√çTICO:** Se seu prompt tiver menos de 1024 tokens, o cache **N√ÉO vai funcionar** com OpenAI (gpt-4o, gpt-4o-mini)!

### O que QUEBRA o cache:

‚ùå Mudar qualquer caractere do prompt
‚ùå Mudar ordem das mensagens
‚ùå Mudar temperature/top_p/etc
‚ùå Adicionar mensagem no MEIO do array

### O que N√ÉO quebra o cache:

‚úÖ Adicionar mensagem no FINAL do array
‚úÖ Usar mesmo system prompt + RAG em conversas diferentes
‚úÖ Trocar apenas a √∫ltima mensagem do usu√°rio

---

## Configura√ß√£o Atual

### Arquitetura Implementada

**Gateway Key:** `vck_...` (uma √∫nica key compartilhada)
**Provider Keys:** Armazenadas no Supabase Vault
**Tracking:** Multi-tenant via `gateway_usage_logs`

**Fluxo:**

```mermaid
graph LR
    A[Cliente] --> B[chatbotFlow]
    B --> C{use_ai_gateway?}
    C -->|Yes| D[callAI via Gateway]
    C -->|No| E[Direct API]
    D --> F[Vercel AI Gateway]
    F --> G[OpenAI/Groq/etc]
    G --> H[Resposta]
    D --> I[Save to gateway_usage_logs]
```

### Arquivos Principais

| Arquivo | Fun√ß√£o |
|---------|--------|
| `src/lib/ai-gateway/index.ts` | Interface principal (`callAI()`) |
| `src/lib/ai-gateway/providers.ts` | Factory de providers |
| `src/lib/ai-gateway/config.ts` | Shared config manager |
| `src/lib/ai-gateway/usage-tracking.ts` | Tracking multi-tenant |
| `src/nodes/generateAIResponse.ts` | Integra√ß√£o com chatbot |
| `src/nodes/fastTrackRouter.ts` | FAQ detection |

### Tabelas do Banco

| Tabela | Prop√≥sito |
|--------|-----------|
| `shared_gateway_config` | Config global (1 registro) |
| `ai_models_registry` | Cat√°logo de modelos + pre√ßos |
| `gateway_usage_logs` | Tracking por request (multi-tenant) |
| `gateway_cache_performance` | M√©tricas de cache |
| `client_budgets` | Controle de budget por cliente |

---

## Benef√≠cios

### 1. Economia de Custos üí∞

**Antes (sem gateway):**
```
100 conversas √ó 1500 tokens input/conversa = 150K tokens
Custo: ~$0.15 (gpt-4o-mini a $0.001/1K)
```

**Depois (com prompt cache):**
```
100 conversas √ó 50 tokens input/conversa (m√©dia) = 5K tokens
Custo: ~$0.005
üí∞ Economia: 97%!
```

### 2. Visibilidade üìä

Dashboard mostra:
- Total de requests
- Tokens consumidos (input/output/cached)
- Custo por per√≠odo
- Lat√™ncia m√©dia (TTFT)
- Requests por modelo

### 3. Fallback Autom√°tico üîÑ

Se OpenAI falhar, tenta outros providers automaticamente.

### 4. Centraliza√ß√£o üéØ

Uma √∫nica interface para m√∫ltiplos providers.

---

## Monitoramento

### Dashboard da Vercel

**URL:** https://vercel.com/[seu-team]/ai/gateway

**M√©tricas importantes:**

| M√©trica | O que significa |
|---------|-----------------|
| **Requests** | Total de chamadas √† IA |
| **Input Tokens** | Tokens enviados (prompts) |
| **Output Tokens** | Tokens gerados (respostas) |
| **Cached Input Tokens** | üí∞ Tokens economizados! |
| **Average TTFT** | Tempo at√© primeiro token |
| **Spend** | Custo total ($) |

### Queries SQL √öteis

**Ver uso por cliente (√∫ltimas 24h):**
```sql
SELECT
  c.name,
  COUNT(*) as requests,
  SUM(gul.input_tokens) as input_tokens,
  SUM(gul.cached_input_tokens) as cached_tokens,
  SUM(gul.output_tokens) as output_tokens,
  SUM(gul.cost_brl) as cost_brl,
  ROUND(
    (SUM(gul.cached_input_tokens)::float / NULLIF(SUM(gul.input_tokens), 0)) * 100,
    2
  ) as cache_hit_rate
FROM gateway_usage_logs gul
JOIN clients c ON gul.client_id = c.id
WHERE gul.created_at > NOW() - INTERVAL '24 hours'
GROUP BY c.name
ORDER BY cost_brl DESC;
```

**Ver economia de cache:**
```sql
SELECT
  DATE(created_at) as date,
  SUM(input_tokens) as total_input,
  SUM(cached_input_tokens) as cached_input,
  ROUND(
    (SUM(cached_input_tokens)::float / NULLIF(SUM(input_tokens), 0)) * 100,
    2
  ) as cache_percentage
FROM gateway_usage_logs
WHERE created_at > NOW() - INTERVAL '7 days'
GROUP BY DATE(created_at)
ORDER BY date DESC;
```

---

## Exemplos Pr√°ticos

### Exemplo 1: Conversa T√≠pica (Com Cache)

**Conversa 1 - Mensagem 1:**
```javascript
messages: [
  {
    role: 'system',
    content: 'Voc√™ √© assistente da Empresa X. Hor√°rio 9-18h.' // 500 tokens
  },
  {
    role: 'user',
    content: 'Qual o hor√°rio?' // 5 tokens
  }
]
// Cobra: 505 tokens input
// Cache: 0 tokens (primeira vez)
```

**Conversa 1 - Mensagem 2:**
```javascript
messages: [
  {
    role: 'system',
    content: 'Voc√™ √© assistente da Empresa X. Hor√°rio 9-18h.' // CACHED!
  },
  {
    role: 'user',
    content: 'Qual o hor√°rio?' // 5 tokens
  },
  {
    role: 'assistant',
    content: 'Atendemos das 9h √†s 18h.' // Hist√≥rico
  },
  {
    role: 'user',
    content: 'E aos s√°bados?' // 5 tokens
  }
]
// Cobra: 10 tokens input (s√≥ as perguntas!)
// Cache: 500 tokens (system + hist√≥rico cacheado)
// üìâ Economia: 98%
```

### Exemplo 2: Fast Track Router (Cache Perfeito)

O Fast Track Router usa **sempre o mesmo prompt**:

```javascript
// Todas as requisi√ß√µes usam:
messages: [
  {
    role: 'system',
    content: 'Voc√™ √© um classificador sem√¢ntico...' // SEMPRE IGUAL
  },
  {
    role: 'user',
    content: 'Cat√°logo de FAQs:\n1. ...\n2. ...\nMensagem: [vari√°vel]'
  }
]
```

**Resultado:**
- System message: SEMPRE cacheado (economiza ~500 tokens)
- Cat√°logo FAQs: SEMPRE cacheado (economiza ~1000 tokens)
- S√≥ paga pelos ~10 tokens da mensagem do usu√°rio
- üí∞ Economia: ~99% nos tokens de input!

---

## FAQ

### 1. Por que n√£o vejo `x-vercel-cache: HIT`?

**R:** Porque `x-vercel-cache` √© para **CDN cache** (edge caching), n√£o para **prompt caching**. O prompt cache acontece no n√≠vel do modelo, n√£o do HTTP.

**Como confirmar que est√° funcionando:**
- ‚úÖ Dashboard mostra "Cached Input Tokens" crescendo
- ‚úÖ Custos por request caem ap√≥s primeira mensagem
- ‚úÖ Query SQL mostra cache_hit_rate > 0%

---

### 2. Cache funciona entre clientes diferentes?

**R:** N√£o! Cada cliente tem:
- System prompts diferentes
- RAG context diferente
- Hist√≥rico separado

Mas **dentro do mesmo cliente**, mensagens sequenciais se beneficiam do cache.

---

### 3. Cache funciona com temperature > 0?

**R:** Sim! Temperature n√£o afeta o cache de **input** (prompt). O que importa √© o prompt ser id√™ntico.

**Por√©m:** Se voc√™ quer respostas **determin√≠sticas** (mesma entrada = mesma sa√≠da), use `temperature: 0`.

---

### 4. Qual a validade do cache?

**R:** Vercel n√£o documenta TTL (Time To Live) expl√≠cito. Na pr√°tica:
- Cache persiste por **minutos a horas**
- Cache de uma conversa ativa (mensagens seguidas) funciona quase sempre
- Cache entre dias diferentes pode expirar

---

### 5. Como desabilitar o gateway para um cliente?

**R:**
```sql
UPDATE clients
SET use_ai_gateway = false
WHERE id = 'client-uuid';
```

O sistema volta a usar chamadas diretas aos providers.

---

### 6. Posso ter Response Cache E Prompt Cache?

**R:** Sim! Voc√™ pode implementar response cache pr√≥prio (Redis) para FAQs, E usar Vercel Gateway para prompt cache.

**Fluxo:**
```javascript
1. Verifica Redis (response cache) ‚Üí HIT? Retorna
2. Se MISS ‚Üí Chama Vercel Gateway (prompt cache economiza tokens)
3. Salva resposta no Redis
```

**Benef√≠cio:** Melhor dos dois mundos (velocidade + economia).

---

### 7. Cache funciona com streaming?

**R:** Sim! Prompt cache funciona mesmo com streaming habilitado.

---

### 8. Preciso de temperature: 0 para cache funcionar?

**R:** **N√£o!** Prompt cache funciona com qualquer temperature. Mas:
- `temperature: 0` = respostas determin√≠sticas
- `temperature > 0` = respostas variadas (mesmo com prompt cacheado)

Para cache de **resposta** (se implementar), a√≠ sim precisa de `temperature: 0`.

---

### 9. Por que meu cache mostra sempre cachedInputTokens = 0?

**R:** Causa mais comum: **Prompt muito curto!**

OpenAI requer **pelo menos 1024 tokens** para ativar prompt caching automaticamente.

**Solu√ß√£o:**
```typescript
// ‚ùå CURTO DEMAIS (~500 tokens) - cache n√£o ativa
const systemPrompt = "Voc√™ √© um assistente...";

// ‚úÖ LONGO SUFICIENTE (1024+ tokens) - cache ativa!
const systemPrompt = `Voc√™ √© um assistente...
[conte√∫do extenso com documenta√ß√£o, FAQs, etc]
[TOTAL: 1024+ tokens]`;
```

**Como verificar:**
1. Conte tokens em https://platform.openai.com/tokenizer
2. System prompt + RAG context devem somar 1024+
3. Teste com endpoint: `GET /api/test/cache`

**Refer√™ncia:** [OpenAI Prompt Caching Docs](https://platform.openai.com/docs/guides/prompt-caching)

---

## Pr√≥ximos Passos

### Fase 1: Monitoramento (Atual)

- [x] Gateway implementado
- [x] Dashboard ativo
- [x] Tracking multi-tenant
- [ ] Validar tracking em produ√ß√£o (24-48h)

### Fase 2: Otimiza√ß√µes (Futuro)

- [ ] Implementar Response Cache (Redis) para FAQs
- [ ] A/B test: temperature 0 vs 0.7
- [ ] Configurar budgets por cliente
- [ ] Alertas de custo alto

### Fase 3: An√°lise (Futuro)

- [ ] Calcular ROI do gateway
- [ ] Identificar clientes com maior economia
- [ ] Otimizar prompts para cache

---

## Recursos

- [Documenta√ß√£o Vercel AI Gateway](https://vercel.com/docs/ai-gateway)
- [Vercel AI SDK](https://sdk.vercel.ai/docs)
- [Prompt Caching (Anthropic)](https://docs.anthropic.com/en/docs/prompt-caching)
- [Dashboard do Projeto](https://vercel.com/ai/gateway)

---

## Contato

D√∫vidas ou sugest√µes sobre o AI Gateway? Abra uma issue ou fale com o time de dev.

---

**‚úÖ Status:** Gateway ativo e economizando tokens!
**üí∞ Economia estimada:** 60-70% nos custos de input
**üìä Dashboard:** https://vercel.com/ai/gateway
