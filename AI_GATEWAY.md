# Vercel AI Gateway - Guia Completo

## üìã √çndice

- [O que √© Vercel AI Gateway?](#o-que-√©-vercel-ai-gateway)
- [Vantagens e Benef√≠cios](#vantagens-e-benef√≠cios)
- [Compara√ß√£o com a Arquitetura Atual](#compara√ß√£o-com-a-arquitetura-atual)
- [Como Funciona](#como-funciona)
- [Guia de Implementa√ß√£o](#guia-de-implementa√ß√£o)
- [Exemplos de C√≥digo](#exemplos-de-c√≥digo)
- [Migra√ß√£o da Arquitetura Atual](#migra√ß√£o-da-arquitetura-atual)
- [Considera√ß√µes Multi-Tenant](#considera√ß√µes-multi-tenant)
- [Seguran√ßa e Boas Pr√°ticas](#seguran√ßa-e-boas-pr√°ticas)
- [Custos e Pricing](#custos-e-pricing)
- [Monitoramento e M√©tricas](#monitoramento-e-m√©tricas)
- [Refer√™ncias](#refer√™ncias)

---

## O que √© Vercel AI Gateway?

**Vercel AI Gateway** √© uma plataforma unificada que simplifica o acesso e gerenciamento de **centenas de modelos de IA** de diversos provedores (OpenAI, Anthropic, Google, Meta, xAI, Groq, Mistral e mais) atrav√©s de um **√∫nico endpoint de API**.

### Problema que Resolve

Atualmente, ao trabalhar com m√∫ltiplos provedores de IA, voc√™ precisa:
- ‚ùå Gerenciar m√∫ltiplos SDKs (OpenAI SDK, Groq SDK, Anthropic SDK, etc.)
- ‚ùå Manter diferentes formatos de API e autentica√ß√£o
- ‚ùå Implementar l√≥gica de fallback manualmente
- ‚ùå Criar seu pr√≥prio sistema de m√©tricas e tracking
- ‚ùå Lidar com rate limiting de cada provedor separadamente
- ‚ùå Implementar cache e retry logic para cada SDK

### Solu√ß√£o

Com Vercel AI Gateway:
- ‚úÖ **Um √∫nico SDK** para todos os provedores
- ‚úÖ **Uma interface unificada** para todos os modelos
- ‚úÖ **Fallback autom√°tico** quando um provedor est√° down
- ‚úÖ **M√©tricas built-in** em dashboard centralizado
- ‚úÖ **Rate limiting global** gerenciado automaticamente
- ‚úÖ **Caching inteligente** para reduzir custos e lat√™ncia
- ‚úÖ **Observabilidade completa** de todos os requests

---

## Vantagens e Benef√≠cios

### 1. üîÑ Acesso Unificado a M√∫ltiplos Modelos

**Benef√≠cio:** Acesse **mais de 100 modelos de IA** com uma √∫nica API.

```typescript
// Antes: C√≥digo diferente para cada provedor
import OpenAI from 'openai'
import Groq from 'groq-sdk'
import Anthropic from '@anthropic-ai/sdk'

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY })
const groq = new Groq({ apiKey: process.env.GROQ_API_KEY })
const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })

// Depois: Uma √∫nica interface
import { streamText } from 'ai'

// Trocar de modelo √© s√≥ mudar o nome
const result = streamText({
  model: 'openai/gpt-4',  // ou 'anthropic/claude-3-5-sonnet', 'groq/llama-3.3-70b'
  messages
})
```

**Vantagem:** Experimente e troque modelos **sem reescrever c√≥digo** - apenas mude o nome do modelo.

### 2. üìä M√©tricas e Observabilidade Built-in

**Benef√≠cio:** Dashboard centralizado com m√©tricas detalhadas de **todos os requests**.

**O que voc√™ obt√©m:**
- üìà Uso por modelo (requests, tokens, custos)
- ‚è±Ô∏è Lat√™ncia m√©dia e P95/P99
- üö® Taxa de erro por provedor
- üí∞ Distribui√ß√£o de custos em tempo real
- üîç Logs granulares de cada request
- üìâ Tend√™ncias de uso ao longo do tempo

**Compara√ß√£o com implementa√ß√£o manual:**

| Recurso | Implementa√ß√£o Atual | Com AI Gateway |
|---------|---------------------|----------------|
| Tracking de tokens | Manual (usageTracking.ts) | ‚úÖ Autom√°tico |
| Custos por modelo | C√°lculo manual | ‚úÖ Dashboard em tempo real |
| Lat√™ncia de requests | Sem tracking | ‚úÖ P50/P95/P99 autom√°ticos |
| Logs de erro | Console logs | ‚úÖ Dashboard centralizado |
| Analytics hist√≥rico | Precisa construir | ‚úÖ Built-in |

### 3. üíæ Caching Inteligente

**Benef√≠cio:** Reduza **custos e lat√™ncia** automaticamente.

**Como funciona:**
- Requests id√™nticos retornam resposta cacheada (lat√™ncia ~20ms)
- Cache gerenciado automaticamente pelo Gateway
- Configur√°vel por modelo e tempo de expira√ß√£o

**Exemplo de economia:**
```
Sem cache:
- 1000 requests id√™nticos/dia = 1000 √ó $0.03 = $30/dia
- Lat√™ncia m√©dia: 800ms

Com cache (hit rate 70%):
- 300 requests reais + 700 cache = 300 √ó $0.03 = $9/dia
- Lat√™ncia m√©dia: 200ms (700 requests em 20ms)
- Economia: 70% de custos + 75% menos lat√™ncia
```

### 4. üîÑ Failover Autom√°tico

**Benef√≠cio:** Alta disponibilidade **sem c√≥digo adicional**.

**Funcionamento:**
- Se OpenAI est√° rate-limited ‚Üí automaticamente tenta Groq
- Se um provedor est√° down ‚Üí redireciona para backup
- Load balancing entre provedores para distribuir carga

```typescript
// Configura√ß√£o de fallback
const result = streamText({
  model: 'openai/gpt-4',
  fallbacks: ['anthropic/claude-3-5-sonnet', 'groq/llama-3.3-70b']
})

// Gateway tenta automaticamente na ordem definida
// Voc√™ n√£o precisa tratar falhas manualmente
```

### 5. üõ°Ô∏è Rate Limiting Global

**Benef√≠cio:** Evite ultrapassar limites de API **automaticamente**.

- Rate limiting inteligente por provedor
- Distribui√ß√£o autom√°tica de requests entre provedores
- Prote√ß√£o contra burst traffic

### 6. üí∞ Transpar√™ncia de Custos

**Benef√≠cio:** Visibilidade total de gastos **em tempo real**.

- Custos por modelo, por cliente, por dia
- Sem markup (Vercel cobra pre√ßos de mercado)
- Suporte a BYOK (Bring Your Own Key) para billing direto
- Alertas de budget configur√°veis

### 7. üöÄ Performance

**Benef√≠cio:** Lat√™ncia ultra-baixa de **~20ms** para gerenciamento.

- Overhead m√≠nimo (20ms) comparado aos requests diretos
- Edge network global da Vercel
- Otimiza√ß√£o de rotas para menor lat√™ncia

---

## Compara√ß√£o com a Arquitetura Atual

### Arquitetura Atual (src/lib/openai.ts + groq.ts)

```typescript
// ‚ùå M√∫ltiplos SDKs para gerenciar
import OpenAI from 'openai'
import Groq from 'groq-sdk'

// ‚ùå L√≥gica de sele√ß√£o manual
const client = config.aiProvider === 'openai' 
  ? new OpenAI({ apiKey }) 
  : new Groq({ apiKey })

// ‚ùå Tracking manual de uso
await trackUsage(clientId, {
  model: response.model,
  inputTokens: response.usage.prompt_tokens,
  outputTokens: response.usage.completion_tokens
})

// ‚ùå Sem fallback autom√°tico
// ‚ùå Sem m√©tricas centralizadas
// ‚ùå Sem cache autom√°tico
```

**Problemas:**
- üî¥ C√≥digo duplicado entre provedores
- üî¥ Sem failover autom√°tico
- üî¥ M√©tricas fragmentadas (precisa consultar Supabase)
- üî¥ Sem cache inteligente
- üî¥ Rate limiting manual
- üî¥ Dif√≠cil adicionar novos provedores

### Arquitetura com AI Gateway

```typescript
// ‚úÖ SDK √∫nico
import { streamText } from 'ai'

// ‚úÖ Sele√ß√£o simplificada
const result = streamText({
  model: `${config.aiProvider}/${config.modelName}`,
  messages,
  // ‚úÖ Fallback autom√°tico
  fallbacks: config.fallbackModels
})

// ‚úÖ M√©tricas autom√°ticas no dashboard Vercel
// ‚úÖ Cache gerenciado automaticamente
// ‚úÖ Rate limiting global
// ‚úÖ Failover sem c√≥digo adicional
```

**Vantagens:**
- üü¢ C√≥digo 70% menor
- üü¢ Failover autom√°tico
- üü¢ M√©tricas em tempo real no dashboard
- üü¢ Cache inteligente built-in
- üü¢ Rate limiting gerenciado
- üü¢ Adicionar provedor = mudar string

---

## Como Funciona

### Fluxo de Request

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Next.js    ‚îÇ  1. Request com model name
‚îÇ  App        ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ
                                     ‚ñº
                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                            ‚îÇ Vercel AI      ‚îÇ
                            ‚îÇ Gateway        ‚îÇ
                            ‚îÇ                ‚îÇ
                            ‚îÇ ‚Ä¢ Routing      ‚îÇ
                            ‚îÇ ‚Ä¢ Cache check  ‚îÇ
                            ‚îÇ ‚Ä¢ Metrics      ‚îÇ
                            ‚îÇ ‚Ä¢ Fallback     ‚îÇ
                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                     ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚ñº                ‚ñº                ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ OpenAI   ‚îÇ    ‚îÇ  Groq    ‚îÇ    ‚îÇAnthropic ‚îÇ
              ‚îÇ GPT-4    ‚îÇ    ‚îÇ Llama 3  ‚îÇ    ‚îÇ Claude   ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Recursos Autom√°ticos

1. **Request Routing:** Gateway encaminha para o provedor correto
2. **Cache Lookup:** Verifica se resposta est√° em cache
3. **Metrics Collection:** Registra tokens, lat√™ncia, custos
4. **Automatic Retry:** Tenta novamente em caso de falha
5. **Fallback:** Troca de provedor se necess√°rio
6. **Response Streaming:** Retorna tokens progressivamente

---

## Guia de Implementa√ß√£o

### Passo 1: Instalar Depend√™ncias

```bash
npm install ai @ai-sdk/react zod
```

**Pacotes:**
- `ai` - Core do Vercel AI SDK
- `@ai-sdk/react` - React hooks para IA (useChat, useCompletion)
- `zod` - Valida√ß√£o de schemas (para tool calls)

### Passo 2: Configurar Vari√°veis de Ambiente

```env
# .env.local

# Vercel AI Gateway (substitui chaves individuais)
AI_GATEWAY_API_KEY=your-vercel-ai-gateway-key

# Ou BYOK (Bring Your Own Key) - billing direto com provedores
OPENAI_API_KEY=sk-...
GROQ_API_KEY=gsk_...
ANTHROPIC_API_KEY=sk-ant-...
```

**Onde obter a chave:**
1. Acesse [Vercel Dashboard](https://vercel.com/dashboard)
2. V√° em **AI Gateway** no menu lateral
3. Clique em **Get API Key**
4. Copie e adicione ao `.env.local`

### Passo 3: Criar API Route

```typescript
// src/app/api/chat/route.ts

import { streamText } from 'ai'

export const runtime = 'edge' // Opcional: edge runtime para menor lat√™ncia
export const dynamic = 'force-dynamic'

export async function POST(req: Request) {
  try {
    const { messages, model = 'openai/gpt-4' } = await req.json()

    const result = streamText({
      model, // Ex: 'groq/llama-3.3-70b', 'anthropic/claude-3-5-sonnet'
      messages,
      temperature: 0.7,
      maxTokens: 1000,
      // Fallback autom√°tico
      fallbacks: [
        'anthropic/claude-3-5-sonnet',
        'groq/llama-3.3-70b'
      ]
    })

    // Retorna stream de resposta
    return result.toDataStreamResponse()
  } catch (error) {
    console.error('AI Gateway error:', error)
    return new Response('Error generating response', { status: 500 })
  }
}
```

### Passo 4: Usar no Frontend

```tsx
// src/components/ChatInterface.tsx

'use client'

import { useChat } from '@ai-sdk/react'

export const ChatInterface = () => {
  const { messages, input, handleInputChange, handleSubmit, isLoading } = useChat({
    api: '/api/chat',
    body: {
      model: 'groq/llama-3.3-70b' // Configur√°vel por cliente
    }
  })

  return (
    <div className="flex flex-col h-screen">
      {/* Messages */}
      <div className="flex-1 overflow-y-auto p-4">
        {messages.map((msg) => (
          <div key={msg.id} className={msg.role === 'user' ? 'text-right' : 'text-left'}>
            <div className="inline-block p-3 rounded-lg bg-gray-100">
              {msg.content}
            </div>
          </div>
        ))}
        {isLoading && <div>IA est√° pensando...</div>}
      </div>

      {/* Input */}
      <form onSubmit={handleSubmit} className="p-4 border-t">
        <input
          value={input}
          onChange={handleInputChange}
          placeholder="Digite sua mensagem..."
          className="w-full p-2 border rounded"
        />
        <button type="submit" className="mt-2 px-4 py-2 bg-blue-500 text-white rounded">
          Enviar
        </button>
      </form>
    </div>
  )
}
```

---

## Exemplos de C√≥digo

### Exemplo 1: Chat Completion com Streaming

```typescript
// src/app/api/chat/route.ts

import { streamText } from 'ai'

export async function POST(req: Request) {
  const { messages, clientId } = await req.json()

  // Buscar configura√ß√£o do cliente do Supabase Vault
  const config = await getClientConfig(clientId)

  const result = streamText({
    model: `${config.aiProvider}/${config.modelName}`,
    messages,
    temperature: config.temperature || 0.7,
    maxTokens: config.maxTokens || 1000,
    systemPrompt: config.systemPrompt,
    
    // Callbacks para tracking custom (opcional)
    onFinish: async ({ usage, response }) => {
      // Ainda pode fazer tracking no Supabase se necess√°rio
      await trackUsageInSupabase(clientId, {
        model: response.model,
        inputTokens: usage.promptTokens,
        outputTokens: usage.completionTokens
      })
    }
  })

  return result.toDataStreamResponse()
}
```

### Exemplo 2: Tool Calls (Sub-agentes)

```typescript
import { streamText, tool } from 'ai'
import { z } from 'zod'

export async function POST(req: Request) {
  const { messages } = await req.json()

  const result = streamText({
    model: 'openai/gpt-4',
    messages,
    tools: {
      // Tool para transferir para humano
      transfer_to_human: tool({
        description: 'Transfer conversation to human agent',
        parameters: z.object({
          reason: z.string().describe('Reason for transfer'),
          urgency: z.enum(['low', 'medium', 'high'])
        }),
        execute: async ({ reason, urgency }) => {
          await createTransferRequest(reason, urgency)
          return { success: true, message: 'Transferido para atendimento humano' }
        }
      }),
      
      // Tool para buscar documentos
      search_knowledge: tool({
        description: 'Search in knowledge base',
        parameters: z.object({
          query: z.string()
        }),
        execute: async ({ query }) => {
          const results = await searchInVectorStore(query)
          return { results }
        }
      })
    }
  })

  return result.toDataStreamResponse()
}
```

### Exemplo 3: Transcri√ß√£o de √Åudio

```typescript
import { transcribeAudio } from 'ai'

export async function POST(req: Request) {
  const formData = await req.formData()
  const audioFile = formData.get('audio') as File

  const result = await transcribeAudio({
    model: 'openai/whisper-1',
    file: audioFile,
    language: 'pt', // Portugu√™s
    responseFormat: 'json'
  })

  return Response.json({
    text: result.text,
    duration: result.duration,
    language: result.language
  })
}
```

### Exemplo 4: An√°lise de Imagem

```typescript
import { generateText } from 'ai'

export async function POST(req: Request) {
  const { imageUrl, prompt } = await req.json()

  const result = await generateText({
    model: 'openai/gpt-4o', // Modelo com vision
    messages: [
      {
        role: 'user',
        content: [
          { type: 'text', text: prompt },
          { type: 'image', image: imageUrl }
        ]
      }
    ]
  })

  return Response.json({ description: result.text })
}
```

---

## Migra√ß√£o da Arquitetura Atual

### Fase 1: Prepara√ß√£o (Sem Breaking Changes)

**Objetivo:** Adicionar AI Gateway como op√ß√£o paralela.

```typescript
// src/lib/ai-gateway.ts (NOVO ARQUIVO)

import { streamText, generateText } from 'ai'

export const generateAIResponseWithGateway = async (
  messages: ChatMessage[],
  config: ClientConfig
) => {
  const result = await streamText({
    model: `${config.aiProvider}/${config.modelName}`,
    messages,
    temperature: config.temperature,
    maxTokens: config.maxTokens,
    systemPrompt: config.systemPrompt
  })

  return result
}
```

**Modificar apenas a API route para testar:**

```typescript
// src/app/api/webhook/[clientId]/route.ts

const USE_AI_GATEWAY = process.env.ENABLE_AI_GATEWAY === 'true' // Feature flag

if (USE_AI_GATEWAY) {
  response = await generateAIResponseWithGateway(messages, config)
} else {
  // Mant√©m l√≥gica atual
  response = await generateAIResponse(messages, config)
}
```

**Vantagens desta abordagem:**
- ‚úÖ Zero downtime
- ‚úÖ Pode testar em staging primeiro
- ‚úÖ Rollback instant√¢neo (mudar feature flag)
- ‚úÖ Comparar performance lado a lado

### Fase 2: Migra√ß√£o Gradual

1. **Migrar 10% do tr√°fego:**
   ```typescript
   const USE_AI_GATEWAY = Math.random() < 0.1 // 10% de chance
   ```

2. **Monitorar m√©tricas:**
   - Lat√™ncia (Gateway vs. direto)
   - Taxa de erro
   - Custos
   - Satisfa√ß√£o de resposta

3. **Aumentar gradualmente:**
   - 10% ‚Üí 25% ‚Üí 50% ‚Üí 100%

### Fase 3: Consolida√ß√£o

**Remover c√≥digo legado ap√≥s valida√ß√£o completa:**

```typescript
// ‚ùå REMOVER (ap√≥s migra√ß√£o 100%)
// src/lib/openai.ts
// src/lib/groq.ts

// ‚úÖ MANTER (simplificado)
// src/lib/ai-gateway.ts (√∫nico arquivo para todos os provedores)
```

**Benef√≠cios ap√≥s migra√ß√£o:**
- üìâ ~500 linhas de c√≥digo a menos
- üöÄ Manuten√ß√£o 70% mais simples
- üìä M√©tricas unificadas
- üí∞ Melhor controle de custos

### Exemplo de C√≥digo Migrado

**Antes (generateAIResponse.ts - 200 linhas):**

```typescript
// L√≥gica complexa de sele√ß√£o de provedor
if (config.aiProvider === 'openai') {
  const client = getOpenAIClient(config.openaiApiKey)
  const response = await client.chat.completions.create({...})
} else if (config.aiProvider === 'groq') {
  const client = getGroqClient(config.groqApiKey)
  const response = await client.chat.completions.create({...})
}

// Tracking manual
await trackUsage(...)

// Sem fallback autom√°tico
// Sem cache
// Sem m√©tricas centralizadas
```

**Depois (ai-gateway.ts - 50 linhas):**

```typescript
import { streamText } from 'ai'

export const generateAIResponse = async (messages, config) => {
  return streamText({
    model: `${config.aiProvider}/${config.modelName}`,
    messages,
    temperature: config.temperature,
    // Gateway cuida de: fallback, cache, metrics, rate limiting
  })
}

// M√©tricas autom√°ticas no dashboard Vercel
// Fallback autom√°tico configurado
// Cache gerenciado pelo Gateway
```

---

## Considera√ß√µes Multi-Tenant

### Problema: Gerenciar API Keys por Cliente

**Arquitetura atual:**
- Cada cliente tem suas pr√≥prias API keys no Supabase Vault
- Diferentes provedores (OpenAI, Groq) por cliente

**Solu√ß√£o com AI Gateway:**

### Op√ß√£o 1: Gateway Key √önica (Recomendado para Custo)

```typescript
// Todos os clientes usam a mesma Gateway Key
// Vercel gerencia billing e voc√™ cobra os clientes

export async function POST(req: Request) {
  const { clientId, messages } = await req.json()
  
  // Buscar configura√ß√£o do cliente (modelo preferido)
  const config = await getClientConfig(clientId)
  
  const result = streamText({
    model: `${config.aiProvider}/${config.modelName}`,
    messages,
    // Gateway Key √∫nica (do env)
    // Vercel cobra voc√™ pelo uso total
    onFinish: async ({ usage }) => {
      // Calcular custo e cobrar cliente
      const cost = calculateCost(usage, config.modelName)
      await billingService.chargeClient(clientId, cost)
    }
  })
  
  return result.toDataStreamResponse()
}
```

**Vantagens:**
- ‚úÖ Simplificado (uma key apenas)
- ‚úÖ Voc√™ controla pricing markup
- ‚úÖ M√©tricas consolidadas

**Desvantagens:**
- ‚ùå Voc√™ assume risco de billing
- ‚ùå Precisa gerenciar cobran√ßas

### Op√ß√£o 2: BYOK (Bring Your Own Key) por Cliente

```typescript
// Cada cliente usa suas pr√≥prias keys
// Billing direto com provedores

export async function POST(req: Request) {
  const { clientId, messages } = await req.json()
  
  const config = await getClientConfig(clientId)
  
  // Passar API key do cliente para o Gateway
  const result = streamText({
    model: `${config.aiProvider}/${config.modelName}`,
    messages,
    // Gateway usa a key do cliente
    apiKey: config.openaiApiKey, // ou groqApiKey
    onFinish: async ({ usage }) => {
      // Apenas tracking (billing j√° √© direto com provedor)
      await trackUsage(clientId, usage)
    }
  })
  
  return result.toDataStreamResponse()
}
```

**Vantagens:**
- ‚úÖ Zero risco de billing
- ‚úÖ Cliente paga diretamente provedor
- ‚úÖ Clientes podem usar seus pr√≥prios cr√©ditos

**Desvantagens:**
- ‚ùå Precisa gerenciar m√∫ltiplas keys
- ‚ùå Clientes precisam criar contas nos provedores

### Recomenda√ß√£o para o Projeto

**H√≠brido:**
- Clientes free/starter ‚Üí Gateway Key √∫nica (voc√™ gerencia)
- Clientes enterprise ‚Üí BYOK (billing direto)

```typescript
const config = await getClientConfig(clientId)

const apiConfig = config.plan === 'enterprise'
  ? { apiKey: config.openaiApiKey } // BYOK
  : {} // Gateway Key padr√£o

const result = streamText({
  model: `${config.aiProvider}/${config.modelName}`,
  messages,
  ...apiConfig
})
```

---

## Seguran√ßa e Boas Pr√°ticas

### 1. Autentica√ß√£o

```typescript
// middleware.ts - Proteger API routes

import { createServerClient } from '@/lib/supabase'

export async function middleware(req: NextRequest) {
  const supabase = createServerClient()
  const { data: { user } } = await supabase.auth.getUser()
  
  if (!user && req.nextUrl.pathname.startsWith('/api/chat')) {
    return new Response('Unauthorized', { status: 401 })
  }
  
  return NextResponse.next()
}
```

### 2. Rate Limiting por Cliente

```typescript
import { Ratelimit } from '@upstash/ratelimit'
import { Redis } from '@upstash/redis'

const ratelimit = new Ratelimit({
  redis: Redis.fromEnv(),
  limiter: Ratelimit.slidingWindow(100, '1 h'), // 100 requests/hora
})

export async function POST(req: Request) {
  const { clientId } = await req.json()
  
  const { success, limit, remaining } = await ratelimit.limit(clientId)
  
  if (!success) {
    return new Response('Rate limit exceeded', { status: 429 })
  }
  
  // Continue com o request...
}
```

### 3. Sanitiza√ß√£o de Input

```typescript
import { z } from 'zod'

const messageSchema = z.object({
  messages: z.array(z.object({
    role: z.enum(['user', 'assistant', 'system']),
    content: z.string().max(4000) // Limite de caracteres
  })),
  clientId: z.string().uuid()
})

export async function POST(req: Request) {
  const body = await req.json()
  
  // Validar input
  const validation = messageSchema.safeParse(body)
  
  if (!validation.success) {
    return new Response('Invalid input', { status: 400 })
  }
  
  const { messages, clientId } = validation.data
  // Continue...
}
```

### 4. Timeout e Error Handling

```typescript
export async function POST(req: Request) {
  try {
    const result = streamText({
      model: 'openai/gpt-4',
      messages,
      maxRetries: 2, // Tenta 2 vezes se falhar
      abortSignal: AbortSignal.timeout(30000), // 30s timeout
      fallbacks: ['anthropic/claude-3-5-sonnet']
    })
    
    return result.toDataStreamResponse()
  } catch (error) {
    if (error.name === 'TimeoutError') {
      return new Response('Request timeout', { status: 408 })
    }
    
    if (error.status === 429) {
      return new Response('Rate limit exceeded', { status: 429 })
    }
    
    // Log error mas n√£o exponha detalhes
    console.error('AI Gateway error:', error)
    return new Response('Error generating response', { status: 500 })
  }
}
```

### 5. Content Moderation

```typescript
import { moderate } from 'ai'

export async function POST(req: Request) {
  const { messages } = await req.json()
  
  // Moderar conte√∫do antes de enviar para IA
  const lastMessage = messages[messages.length - 1].content
  
  const moderation = await moderate({
    model: 'openai/text-moderation-latest',
    input: lastMessage
  })
  
  if (moderation.flagged) {
    return new Response('Content violates policy', { 
      status: 400,
      body: JSON.stringify({ categories: moderation.categories })
    })
  }
  
  // Continue com request...
}
```

---

## Custos e Pricing

### Modelo de Pricing do Vercel AI Gateway

**Gateway:**
- ‚úÖ **Sem markup** sobre pre√ßos dos provedores
- ‚úÖ **Billing transparente** - paga apenas o que usa
- ‚úÖ **BYOK suportado** - use suas pr√≥prias keys

**Custos t√≠picos (por 1M tokens):**

| Provedor | Modelo | Input | Output |
|----------|--------|-------|--------|
| OpenAI | GPT-4o | $2.50 | $10.00 |
| OpenAI | GPT-4o-mini | $0.15 | $0.60 |
| Anthropic | Claude 3.5 Sonnet | $3.00 | $15.00 |
| Groq | Llama 3.3 70B | $0.59 | $0.79 |
| Groq | Llama 3.1 8B | $0.05 | $0.08 |

### Estimativa de Economia com Cache

**Cen√°rio:** 10.000 requests/dia, 70% hit rate de cache

```
Sem cache:
- 10.000 requests √ó 1000 tokens √ó $0.001/1k tokens = $10/dia
- Total: $300/m√™s

Com cache (70% hit):
- 3.000 requests reais √ó 1000 tokens √ó $0.001/1k tokens = $3/dia
- Total: $90/m√™s
- Economia: $210/m√™s (70%)
```

### Compara√ß√£o de Custos: Implementa√ß√£o Atual vs. AI Gateway

**Implementa√ß√£o Atual:**
- API keys diretas com provedores
- Sem cache autom√°tico
- Tracking manual (custos de infraestrutura Supabase)
- Sem otimiza√ß√£o de rotas

**Com AI Gateway:**
- Billing consolidado
- Cache autom√°tico (economia ~50-70%)
- M√©tricas inclu√≠das
- Roteamento otimizado (menor lat√™ncia = menos re-requests)

**Exemplo para 100k requests/m√™s:**

| Item | Atual | Com Gateway | Economia |
|------|-------|-------------|----------|
| API calls | $500 | $200 | $300 (cache) |
| Tracking infra | $50 | $0 | $50 |
| M√©tricas/logs | $30 | $0 | $30 |
| **Total** | **$580** | **$200** | **$380 (65%)** |

---

## Monitoramento e M√©tricas

### Dashboard do Vercel AI Gateway

Acesse: [Vercel Dashboard ‚Üí AI Gateway](https://vercel.com/dashboard/ai-gateway)

**M√©tricas dispon√≠veis:**

#### 1. Overview
- üìä Total de requests (√∫ltimas 24h, 7d, 30d)
- üí∞ Custos totais e por modelo
- ‚ö° Lat√™ncia m√©dia (P50, P95, P99)
- üéØ Taxa de sucesso vs. erros

#### 2. Por Modelo
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Model Performance                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ openai/gpt-4         12,450 requests    ‚îÇ
‚îÇ ‚îú‚îÄ Avg latency: 850ms                   ‚îÇ
‚îÇ ‚îú‚îÄ Success rate: 99.2%                  ‚îÇ
‚îÇ ‚îî‚îÄ Cost: $245.50                        ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ groq/llama-3.3-70b   8,320 requests     ‚îÇ
‚îÇ ‚îú‚îÄ Avg latency: 320ms                   ‚îÇ
‚îÇ ‚îú‚îÄ Success rate: 99.8%                  ‚îÇ
‚îÇ ‚îî‚îÄ Cost: $12.40                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### 3. Cache Performance
- üéØ Hit rate (%)
- üíæ Requests servidos do cache
- üí∞ Economia estimada

#### 4. Logs Detalhados
- Cada request individual
- Timestamps, lat√™ncia, tokens usados
- Erros e stack traces
- User/client ID (se configurado)

### Integra√ß√£o com Ferramentas Existentes

**Continuar usando Supabase Analytics:**

```typescript
// src/app/api/chat/route.ts

import { trackUsage } from '@/lib/usageTracking'

const result = streamText({
  model: 'openai/gpt-4',
  messages,
  onFinish: async ({ usage, response }) => {
    // Ainda pode salvar no Supabase para analytics custom
    await trackUsage(clientId, {
      model: response.model,
      inputTokens: usage.promptTokens,
      outputTokens: usage.completionTokens,
      cost: calculateCost(usage, response.model),
      conversationId,
      messageId
    })
  }
})
```

**Benef√≠cios de dual tracking:**
- ‚úÖ Vercel: M√©tricas t√©cnicas (lat√™ncia, erros, cache)
- ‚úÖ Supabase: Analytics de neg√≥cio (uso por cliente, ROI, trends)

---

## Pr√≥ximos Passos Recomendados

### Fase 1: Explora√ß√£o (1-2 dias)
1. ‚úÖ Criar conta Vercel (se ainda n√£o tem)
2. ‚úÖ Ativar AI Gateway no dashboard
3. ‚úÖ Obter API key de teste
4. ‚úÖ Testar com exemplo simples

### Fase 2: Proof of Concept (1 semana)
1. ‚úÖ Criar branch `feature/ai-gateway-poc`
2. ‚úÖ Implementar API route paralela (`/api/chat-gateway`)
3. ‚úÖ Testar com 1 cliente em staging
4. ‚úÖ Comparar m√©tricas: lat√™ncia, custos, taxa de erro

### Fase 3: Migra√ß√£o Gradual (2-3 semanas)
1. ‚úÖ Feature flag para habilitar Gateway por cliente
2. ‚úÖ Migrar 10% do tr√°fego
3. ‚úÖ Monitorar m√©tricas por 1 semana
4. ‚úÖ Aumentar para 50% se tudo ok
5. ‚úÖ Migrar 100% ap√≥s valida√ß√£o

### Fase 4: Otimiza√ß√£o (ongoing)
1. ‚úÖ Configurar cache policies
2. ‚úÖ Ajustar fallback strategies
3. ‚úÖ Implementar alertas de budget
4. ‚úÖ Remover c√≥digo legado

---

## Refer√™ncias

### Documenta√ß√£o Oficial
- [Vercel AI Gateway - Docs](https://vercel.com/docs/ai-gateway)
- [Vercel AI SDK - Docs](https://ai-sdk.dev/)
- [AI Gateway Providers](https://ai-sdk.dev/providers/ai-sdk-providers/ai-gateway)
- [Next.js App Router Integration](https://ai-sdk.dev/docs/getting-started/nextjs-app-router)

### Templates e Exemplos
- [Vercel AI Gateway Demo](https://vercel.com/templates/next.js/vercel-ai-gateway-demo)
- [AI Chatbot Template](https://vercel.com/templates/next.js/nextjs-ai-chatbot)

### Artigos e Tutoriais
- [Building an AI Chatbot with Vercel AI SDK](https://benseymour.com/blog/2025-09-13-Building-an-AI-Chatbot-with-Vercel-AI-SDK-and-AI-Gateway)
- [Real-time AI in Next.js with Vercel AI SDK](https://blog.logrocket.com/nextjs-vercel-ai-sdk-streaming/)
- [Vercel Introduces AI Gateway - InfoQ](https://www.infoq.com/news/2025/09/vercel-ai-gateway/)

### Comunidade
- [Vercel AI SDK GitHub](https://github.com/vercel/ai)
- [Discord da Vercel](https://vercel.com/discord)

---

## Perguntas Frequentes

### P: Preciso migrar tudo de uma vez?
**R:** N√£o! Use feature flags para migrar gradualmente. Pode come√ßar com 10% do tr√°fego.

### P: Vou perder as m√©tricas atuais do Supabase?
**R:** N√£o. Pode continuar salvando no Supabase em paralelo para analytics de neg√≥cio.

### P: Como funcionam os custos com m√∫ltiplos clientes?
**R:** Voc√™ pode usar uma Gateway Key √∫nica (voc√™ gerencia billing) ou BYOK (cliente paga direto).

### P: E se eu quiser voltar para a implementa√ß√£o atual?
**R:** Basta mudar a feature flag. Zero vendor lock-in.

### P: Cache funciona com mensagens diferentes?
**R:** Cache √© inteligente - funciona para requests id√™nticos (mesmo prompt + contexto).

### P: Como funciona fallback autom√°tico?
**R:** Configure array de fallbacks. Gateway tenta na ordem se o primeiro falhar.

### P: Posso usar modelos locais/custom?
**R:** Sim, AI Gateway suporta endpoints custom al√©m dos provedores principais.

---

## Conclus√£o

**Vercel AI Gateway √© uma solu√ß√£o moderna e poderosa** para gerenciar m√∫ltiplos modelos de IA em produ√ß√£o. Para este projeto, os principais benef√≠cios seriam:

### Benef√≠cios Principais para o Projeto

1. **üîÑ Simplifica√ß√£o de C√≥digo**
   - Redu√ß√£o de ~500 linhas de c√≥digo
   - Manuten√ß√£o 70% mais simples
   - Menos SDKs para gerenciar

2. **üìä M√©tricas Unificadas**
   - Dashboard centralizado
   - Visibilidade total de custos
   - Analytics em tempo real

3. **üí∞ Redu√ß√£o de Custos**
   - Cache autom√°tico (economia ~50-70%)
   - Roteamento inteligente
   - Sem custos de infraestrutura de tracking

4. **üõ°Ô∏è Maior Confiabilidade**
   - Failover autom√°tico
   - Rate limiting global
   - Alta disponibilidade (99.9%+)

5. **üöÄ Melhor Performance**
   - Lat√™ncia otimizada
   - Edge network global
   - Streaming eficiente

### Recomenda√ß√£o Final

**‚úÖ Recomendo a ado√ß√£o** do Vercel AI Gateway para este projeto, com migra√ß√£o gradual:

- **Curto prazo (1 m√™s):** POC com 10% do tr√°fego
- **M√©dio prazo (3 meses):** Migra√ß√£o completa
- **Longo prazo:** Otimiza√ß√£o e economia cont√≠nua

O investimento inicial de tempo √© compensado pela **redu√ß√£o de manuten√ß√£o, melhor observabilidade e economia de custos** a longo prazo.

---

**Documento criado em:** 2024-12-11
**√öltima atualiza√ß√£o:** 2024-12-11
**Vers√£o:** 1.0
